W0622 06:34:23.787000 65335 site-packages/torch/distributed/run.py:793] 
W0622 06:34:23.787000 65335 site-packages/torch/distributed/run.py:793] *****************************************
W0622 06:34:23.787000 65335 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0622 06:34:23.787000 65335 site-packages/torch/distributed/run.py:793] *****************************************
wandb: Currently logged in as: leonli66 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:[W622 06:35:06.647146504 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W622 06:35:06.691909562 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W622 06:35:06.700110896 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W622 06:35:06.703353177 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W622 06:35:06.704928353 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W622 06:35:06.778416716 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W622 06:35:06.798313389 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: creating run
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /workspace/bagel-training/wandb/run-20250622_063504-h200-zebra-cot-20250622_063422-run0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run h200-zebra-cot-20250622_063422
wandb: ‚≠êÔ∏è View project at https://wandb.ai/leonli66/zebra-cot
wandb: üöÄ View run at https://wandb.ai/leonli66/zebra-cot/runs/h200-zebra-cot-20250622_063422-run0
[rank0]:[W622 06:35:16.313493674 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[[34m2025-06-22 06:35:35[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results/', checkpoint_dir='results/checkpoints/', wandb_project='zebra-cot', wandb_name='h200-zebra-cot-20250622_063422', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/dev/shm/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=50, total_steps=10000, warmup_steps=100, lr_scheduler='cosine', lr=2e-05, min_lr=1e-06, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=60000, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-06-22 06:35:35[0m] Model arguments ModelArguments(model_path='/dev/shm/models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-06-22 06:35:35[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=60000, max_num_tokens=60000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-06-22 06:38:24[0m] Loading checkpoint from results/checkpoints/0000200.
[[34m2025-06-22 06:38:36[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-06-22 06:39:38[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-06-22 06:40:31[0m] Training for 10000 steps, starting at 201...
[[34m2025-06-22 06:41:43[0m] (step=0000201) Train Loss mse: 0.2449, Train Loss ce: 0.5183, Train Steps/Sec: 0.01, 
[[34m2025-06-22 06:42:12[0m] (step=0000202) Train Loss mse: 0.1511, Train Loss ce: 0.4530, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:42:41[0m] (step=0000203) Train Loss mse: 0.1830, Train Loss ce: 0.5142, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:43:17[0m] (step=0000204) Train Loss mse: 0.1677, Train Loss ce: 0.4317, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:43:41[0m] (step=0000205) Train Loss mse: 0.1462, Train Loss ce: 0.4108, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:44:10[0m] (step=0000206) Train Loss mse: 0.1697, Train Loss ce: 0.4762, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:44:37[0m] (step=0000207) Train Loss mse: 0.2034, Train Loss ce: 0.4769, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:44:58[0m] (step=0000208) Train Loss mse: 0.1829, Train Loss ce: 0.5003, Train Steps/Sec: 0.05, 
[[34m2025-06-22 06:45:29[0m] (step=0000209) Train Loss mse: 0.1783, Train Loss ce: 0.4780, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:45:56[0m] (step=0000210) Train Loss mse: 0.1492, Train Loss ce: 0.5313, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:46:20[0m] (step=0000211) Train Loss mse: 0.1324, Train Loss ce: 0.5418, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:46:40[0m] (step=0000212) Train Loss mse: 0.1744, Train Loss ce: 0.5270, Train Steps/Sec: 0.05, 
[[34m2025-06-22 06:47:13[0m] (step=0000213) Train Loss mse: 0.1848, Train Loss ce: 0.4981, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:47:40[0m] (step=0000214) Train Loss mse: 0.2012, Train Loss ce: 0.4513, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:48:05[0m] (step=0000215) Train Loss mse: 0.1942, Train Loss ce: 0.4520, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:48:42[0m] (step=0000216) Train Loss mse: 0.1373, Train Loss ce: 0.5473, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:49:04[0m] (step=0000217) Train Loss mse: 0.1890, Train Loss ce: 0.5224, Train Steps/Sec: 0.05, 
[[34m2025-06-22 06:49:38[0m] (step=0000218) Train Loss mse: 0.1778, Train Loss ce: 0.4570, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:50:12[0m] (step=0000219) Train Loss mse: 0.1509, Train Loss ce: 0.4926, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:50:35[0m] (step=0000220) Train Loss mse: 0.1803, Train Loss ce: 0.4304, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:50:59[0m] (step=0000221) Train Loss mse: 0.1962, Train Loss ce: 0.4734, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:51:20[0m] (step=0000222) Train Loss mse: 0.2151, Train Loss ce: 0.4135, Train Steps/Sec: 0.05, 
[[34m2025-06-22 06:51:43[0m] (step=0000223) Train Loss mse: 0.1857, Train Loss ce: 0.5082, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:52:10[0m] (step=0000224) Train Loss mse: 0.1748, Train Loss ce: 0.5058, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:52:35[0m] (step=0000225) Train Loss mse: 0.2150, Train Loss ce: 0.5154, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:53:12[0m] (step=0000226) Train Loss mse: 0.1869, Train Loss ce: 0.4774, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:53:40[0m] (step=0000227) Train Loss mse: 0.1459, Train Loss ce: 0.4451, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:54:07[0m] (step=0000228) Train Loss mse: 0.1657, Train Loss ce: 0.4226, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:54:31[0m] (step=0000229) Train Loss mse: 0.2234, Train Loss ce: 0.4879, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:55:09[0m] (step=0000230) Train Loss mse: 0.1631, Train Loss ce: 0.4352, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:55:38[0m] (step=0000231) Train Loss mse: 0.1704, Train Loss ce: 0.4530, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:56:03[0m] (step=0000232) Train Loss mse: 0.2134, Train Loss ce: 0.4493, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:56:31[0m] (step=0000233) Train Loss mse: 0.1610, Train Loss ce: 0.5141, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:57:02[0m] (step=0000234) Train Loss mse: 0.1954, Train Loss ce: 0.4810, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:57:25[0m] (step=0000235) Train Loss mse: 0.1737, Train Loss ce: 0.5251, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:57:48[0m] (step=0000236) Train Loss mse: 0.1753, Train Loss ce: 0.3961, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:58:15[0m] (step=0000237) Train Loss mse: 0.2014, Train Loss ce: 0.4671, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:58:49[0m] (step=0000238) Train Loss mse: 0.2081, Train Loss ce: 0.4341, Train Steps/Sec: 0.03, 
[[34m2025-06-22 06:59:12[0m] (step=0000239) Train Loss mse: 0.2186, Train Loss ce: 0.5634, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:59:35[0m] (step=0000240) Train Loss mse: 0.1952, Train Loss ce: 0.4756, Train Steps/Sec: 0.04, 
[[34m2025-06-22 06:59:59[0m] (step=0000241) Train Loss mse: 0.1696, Train Loss ce: 0.4995, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:00:34[0m] (step=0000242) Train Loss mse: 0.1407, Train Loss ce: 0.4305, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:01:04[0m] (step=0000243) Train Loss mse: 0.1958, Train Loss ce: 0.4027, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:01:46[0m] (step=0000244) Train Loss mse: 0.1077, Train Loss ce: 0.4620, Train Steps/Sec: 0.02, 
[[34m2025-06-22 07:02:20[0m] (step=0000245) Train Loss mse: 0.2222, Train Loss ce: 0.3932, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:02:52[0m] (step=0000246) Train Loss mse: 0.1859, Train Loss ce: 0.4991, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:03:18[0m] (step=0000247) Train Loss mse: 0.2202, Train Loss ce: 0.4431, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:03:50[0m] (step=0000248) Train Loss mse: 0.1492, Train Loss ce: 0.4799, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:04:20[0m] (step=0000249) Train Loss mse: 0.2018, Train Loss ce: 0.4227, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:04:47[0m] (step=0000250) Train Loss mse: 0.1676, Train Loss ce: 0.5156, Train Steps/Sec: 0.04, 
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[[34m2025-06-22 07:04:48[0m] Saving checkpoint to results/checkpoints/0000250.
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[[34m2025-06-22 07:09:15[0m] Sorted checkpoint directories: ['0000140', '0000150', '0000200', '0000250']
[[34m2025-06-22 07:09:31[0m] Deleted old checkpoint folder: results/checkpoints/0000140
[[34m2025-06-22 07:10:02[0m] (step=0000251) Train Loss mse: 0.1741, Train Loss ce: 0.5459, Train Steps/Sec: 0.00, 
[[34m2025-06-22 07:10:33[0m] (step=0000252) Train Loss mse: 0.1456, Train Loss ce: 0.4425, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:11:04[0m] (step=0000253) Train Loss mse: 0.1810, Train Loss ce: 0.4387, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:11:38[0m] (step=0000254) Train Loss mse: 0.1971, Train Loss ce: 0.4358, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:12:11[0m] (step=0000255) Train Loss mse: 0.1922, Train Loss ce: 0.4332, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:12:37[0m] (step=0000256) Train Loss mse: 0.1382, Train Loss ce: 0.5803, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:13:08[0m] (step=0000257) Train Loss mse: 0.2231, Train Loss ce: 0.4526, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:13:48[0m] (step=0000258) Train Loss mse: 0.1602, Train Loss ce: 0.5274, Train Steps/Sec: 0.02, 
[[34m2025-06-22 07:14:18[0m] (step=0000259) Train Loss mse: 0.1737, Train Loss ce: 0.3943, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:14:49[0m] (step=0000260) Train Loss mse: 0.1533, Train Loss ce: 0.3646, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:15:18[0m] (step=0000261) Train Loss mse: 0.1228, Train Loss ce: 0.4704, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:15:52[0m] (step=0000262) Train Loss mse: 0.1534, Train Loss ce: 0.4574, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:16:14[0m] (step=0000263) Train Loss mse: 0.1666, Train Loss ce: 0.5245, Train Steps/Sec: 0.05, 
[[34m2025-06-22 07:16:44[0m] (step=0000264) Train Loss mse: 0.2149, Train Loss ce: 0.4001, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:17:14[0m] (step=0000265) Train Loss mse: 0.1598, Train Loss ce: 0.5534, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:17:48[0m] (step=0000266) Train Loss mse: 0.1854, Train Loss ce: 0.5912, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:18:17[0m] (step=0000267) Train Loss mse: 0.1619, Train Loss ce: 0.3151, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:18:40[0m] (step=0000268) Train Loss mse: 0.1866, Train Loss ce: 0.5377, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:19:05[0m] (step=0000269) Train Loss mse: 0.1548, Train Loss ce: 0.4386, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:19:28[0m] (step=0000270) Train Loss mse: 0.1937, Train Loss ce: 0.4881, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:20:02[0m] (step=0000271) Train Loss mse: 0.1295, Train Loss ce: 0.5208, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:20:33[0m] (step=0000272) Train Loss mse: 0.1504, Train Loss ce: 0.4751, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:21:00[0m] (step=0000273) Train Loss mse: 0.1654, Train Loss ce: 0.5274, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:21:26[0m] (step=0000274) Train Loss mse: 0.1484, Train Loss ce: 0.4001, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:21:54[0m] (step=0000275) Train Loss mse: 0.1395, Train Loss ce: 0.5299, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:22:21[0m] (step=0000276) Train Loss mse: 0.1717, Train Loss ce: 0.4623, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:22:45[0m] (step=0000277) Train Loss mse: 0.1937, Train Loss ce: 0.4959, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:23:14[0m] (step=0000278) Train Loss mse: 0.1824, Train Loss ce: 0.5328, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:23:41[0m] (step=0000279) Train Loss mse: 0.2263, Train Loss ce: 0.4979, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:24:16[0m] (step=0000280) Train Loss mse: 0.1991, Train Loss ce: 0.4719, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:24:43[0m] (step=0000281) Train Loss mse: 0.1672, Train Loss ce: 0.3693, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:25:13[0m] (step=0000282) Train Loss mse: 0.2041, Train Loss ce: 0.5720, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:25:38[0m] (step=0000283) Train Loss mse: 0.1853, Train Loss ce: 0.4690, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:26:00[0m] (step=0000284) Train Loss mse: 0.1909, Train Loss ce: 0.5278, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:26:42[0m] (step=0000285) Train Loss mse: 0.1512, Train Loss ce: 0.4186, Train Steps/Sec: 0.02, 
[[34m2025-06-22 07:27:09[0m] (step=0000286) Train Loss mse: 0.1571, Train Loss ce: 0.5069, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:27:34[0m] (step=0000287) Train Loss mse: 0.1794, Train Loss ce: 0.4866, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:28:11[0m] (step=0000288) Train Loss mse: 0.1602, Train Loss ce: 0.3834, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:28:43[0m] (step=0000289) Train Loss mse: 0.1521, Train Loss ce: 0.5573, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:29:18[0m] (step=0000290) Train Loss mse: 0.1545, Train Loss ce: 0.4644, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:29:53[0m] (step=0000291) Train Loss mse: 0.2221, Train Loss ce: 0.5164, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:30:19[0m] (step=0000292) Train Loss mse: 0.1962, Train Loss ce: 0.4818, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:30:52[0m] (step=0000293) Train Loss mse: 0.1955, Train Loss ce: 0.4689, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:31:18[0m] (step=0000294) Train Loss mse: 0.1476, Train Loss ce: 0.4573, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:31:49[0m] (step=0000295) Train Loss mse: 0.1596, Train Loss ce: 0.4732, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:32:21[0m] (step=0000296) Train Loss mse: 0.1848, Train Loss ce: 0.4780, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:32:50[0m] (step=0000297) Train Loss mse: 0.2040, Train Loss ce: 0.3875, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:33:24[0m] (step=0000298) Train Loss mse: 0.1802, Train Loss ce: 0.4687, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:33:54[0m] (step=0000299) Train Loss mse: 0.2111, Train Loss ce: 0.4920, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:34:34[0m] (step=0000300) Train Loss mse: 0.1333, Train Loss ce: 0.4767, Train Steps/Sec: 0.02, 
[[34m2025-06-22 07:34:34[0m] Saving checkpoint to results/checkpoints/0000300.
[[34m2025-06-22 07:39:28[0m] Sorted checkpoint directories: ['0000150', '0000200', '0000250', '0000300']
[[34m2025-06-22 07:39:33[0m] Deleted old checkpoint folder: results/checkpoints/0000150
[[34m2025-06-22 07:40:00[0m] (step=0000301) Train Loss mse: 0.1719, Train Loss ce: 0.4003, Train Steps/Sec: 0.00, 
[[34m2025-06-22 07:40:28[0m] (step=0000302) Train Loss mse: 0.2059, Train Loss ce: 0.3883, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:40:55[0m] (step=0000303) Train Loss mse: 0.1952, Train Loss ce: 0.4928, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:41:20[0m] (step=0000304) Train Loss mse: 0.1623, Train Loss ce: 0.5066, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:41:43[0m] (step=0000305) Train Loss mse: 0.1279, Train Loss ce: 0.4994, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:42:10[0m] (step=0000306) Train Loss mse: 0.2084, Train Loss ce: 0.4672, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:42:47[0m] (step=0000307) Train Loss mse: 0.1246, Train Loss ce: 0.4338, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:43:15[0m] (step=0000308) Train Loss mse: 0.1875, Train Loss ce: 0.3930, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:43:42[0m] (step=0000309) Train Loss mse: 0.1858, Train Loss ce: 0.5006, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:44:17[0m] (step=0000310) Train Loss mse: 0.1617, Train Loss ce: 0.4259, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:44:54[0m] (step=0000311) Train Loss mse: 0.1997, Train Loss ce: 0.4627, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:45:21[0m] (step=0000312) Train Loss mse: 0.1581, Train Loss ce: 0.5159, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:45:52[0m] (step=0000313) Train Loss mse: 0.2012, Train Loss ce: 0.4240, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:46:16[0m] (step=0000314) Train Loss mse: 0.1516, Train Loss ce: 0.4737, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:46:43[0m] (step=0000315) Train Loss mse: 0.1200, Train Loss ce: 0.4740, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:47:14[0m] (step=0000316) Train Loss mse: 0.1635, Train Loss ce: 0.4456, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:47:46[0m] (step=0000317) Train Loss mse: 0.1522, Train Loss ce: 0.5342, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:48:13[0m] (step=0000318) Train Loss mse: 0.1666, Train Loss ce: 0.4615, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:48:43[0m] (step=0000319) Train Loss mse: 0.1706, Train Loss ce: 0.4980, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:49:13[0m] (step=0000320) Train Loss mse: 0.1740, Train Loss ce: 0.3985, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:49:44[0m] (step=0000321) Train Loss mse: 0.1765, Train Loss ce: 0.3979, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:50:17[0m] (step=0000322) Train Loss mse: 0.1412, Train Loss ce: 0.3934, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:50:53[0m] (step=0000323) Train Loss mse: 0.1726, Train Loss ce: 0.4415, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:51:22[0m] (step=0000324) Train Loss mse: 0.1580, Train Loss ce: 0.3129, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:51:54[0m] (step=0000325) Train Loss mse: 0.1446, Train Loss ce: 0.3779, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:52:20[0m] (step=0000326) Train Loss mse: 0.1816, Train Loss ce: 0.4343, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:52:47[0m] (step=0000327) Train Loss mse: 0.1659, Train Loss ce: 0.4327, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:53:13[0m] (step=0000328) Train Loss mse: 0.1839, Train Loss ce: 0.4915, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:53:38[0m] (step=0000329) Train Loss mse: 0.2203, Train Loss ce: 0.5474, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:54:12[0m] (step=0000330) Train Loss mse: 0.1927, Train Loss ce: 0.5001, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:54:44[0m] (step=0000331) Train Loss mse: 0.1995, Train Loss ce: 0.4034, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:55:20[0m] (step=0000332) Train Loss mse: 0.1888, Train Loss ce: 0.5074, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:55:45[0m] (step=0000333) Train Loss mse: 0.1332, Train Loss ce: 0.3545, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:56:29[0m] (step=0000334) Train Loss mse: 0.2491, Train Loss ce: 0.4582, Train Steps/Sec: 0.02, 
[[34m2025-06-22 07:57:03[0m] (step=0000335) Train Loss mse: 0.1995, Train Loss ce: 0.4797, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:57:32[0m] (step=0000336) Train Loss mse: 0.1934, Train Loss ce: 0.4712, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:57:59[0m] (step=0000337) Train Loss mse: 0.1538, Train Loss ce: 0.4785, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:58:23[0m] (step=0000338) Train Loss mse: 0.1840, Train Loss ce: 0.4882, Train Steps/Sec: 0.04, 
[[34m2025-06-22 07:58:53[0m] (step=0000339) Train Loss mse: 0.1724, Train Loss ce: 0.3899, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:59:27[0m] (step=0000340) Train Loss mse: 0.1801, Train Loss ce: 0.4609, Train Steps/Sec: 0.03, 
[[34m2025-06-22 07:59:53[0m] (step=0000341) Train Loss mse: 0.1517, Train Loss ce: 0.4445, Train Steps/Sec: 0.04, 
