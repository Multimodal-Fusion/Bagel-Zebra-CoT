W0623 05:52:10.914000 190557 site-packages/torch/distributed/run.py:793] 
W0623 05:52:10.914000 190557 site-packages/torch/distributed/run.py:793] *****************************************
W0623 05:52:10.914000 190557 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0623 05:52:10.914000 190557 site-packages/torch/distributed/run.py:793] *****************************************
wandb: Currently logged in as: leonli66 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank2]:[W623 05:52:54.540268257 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W623 05:52:54.541359223 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W623 05:52:54.549893058 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W623 05:52:54.580096972 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W623 05:52:54.584036287 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W623 05:52:54.601002499 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W623 05:52:54.660144716 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: creating run
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /workspace/bagel-training/wandb/run-20250623_055252-h200-zebra-cot-20250623_055209-run0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run h200-zebra-cot-20250623_055209
wandb: ‚≠êÔ∏è View project at https://wandb.ai/leonli66/zebra-cot
wandb: üöÄ View run at https://wandb.ai/leonli66/zebra-cot/runs/h200-zebra-cot-20250623_055209-run0
[rank0]:[W623 05:53:03.775878816 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[[34m2025-06-23 05:53:22[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results/', checkpoint_dir='results/checkpoints/', wandb_project='zebra-cot', wandb_name='h200-zebra-cot-20250623_055209', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/dev/shm/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=50, total_steps=5000, warmup_steps=50, lr_scheduler='cosine', lr=2e-05, min_lr=1e-06, beta1=0.9, beta2=0.95, eps=1e-08, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=60000, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-06-23 05:53:22[0m] Model arguments ModelArguments(model_path='/dev/shm/models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-06-23 05:53:22[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=60000, max_num_tokens=60000, prefer_buffer_before=30000, max_buffer_size=50, data_seed=42)
[[34m2025-06-23 05:56:15[0m] Loading checkpoint from results/checkpoints/0001200.
[[34m2025-06-23 05:56:29[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-06-23 05:58:02[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-06-23 05:58:50[0m] Training for 5000 steps, starting at 1201...
[[34m2025-06-23 05:59:51[0m] (step=0001201) Train Loss mse: 0.1647, Train Loss ce: 0.4438, Train Steps/Sec: 0.02, 
[[34m2025-06-23 06:00:21[0m] (step=0001202) Train Loss mse: 0.1309, Train Loss ce: 0.3562, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:00:50[0m] (step=0001203) Train Loss mse: 0.1708, Train Loss ce: 0.3494, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:01:24[0m] (step=0001204) Train Loss mse: 0.1753, Train Loss ce: 0.3835, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:01:48[0m] (step=0001205) Train Loss mse: 0.1490, Train Loss ce: 0.4150, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:02:12[0m] (step=0001206) Train Loss mse: 0.1873, Train Loss ce: 0.4304, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:02:42[0m] (step=0001207) Train Loss mse: 0.1654, Train Loss ce: 0.4565, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:03:07[0m] (step=0001208) Train Loss mse: 0.2001, Train Loss ce: 0.3774, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:03:39[0m] (step=0001209) Train Loss mse: 0.2361, Train Loss ce: 0.4767, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:04:05[0m] (step=0001210) Train Loss mse: 0.1398, Train Loss ce: 0.4070, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:04:37[0m] (step=0001211) Train Loss mse: 0.1580, Train Loss ce: 0.4065, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:05:10[0m] (step=0001212) Train Loss mse: 0.2041, Train Loss ce: 0.3977, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:05:53[0m] (step=0001213) Train Loss mse: 0.1367, Train Loss ce: 0.4138, Train Steps/Sec: 0.02, 
[[34m2025-06-23 06:06:24[0m] (step=0001214) Train Loss mse: 0.1738, Train Loss ce: 0.4175, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:06:53[0m] (step=0001215) Train Loss mse: 0.1813, Train Loss ce: 0.4847, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:07:24[0m] (step=0001216) Train Loss mse: 0.1751, Train Loss ce: 0.4132, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:07:57[0m] (step=0001217) Train Loss mse: 0.1775, Train Loss ce: 0.4241, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:08:27[0m] (step=0001218) Train Loss mse: 0.1767, Train Loss ce: 0.3543, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:08:55[0m] (step=0001219) Train Loss mse: 0.1608, Train Loss ce: 0.3515, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:09:28[0m] (step=0001220) Train Loss mse: 0.2222, Train Loss ce: 0.3757, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:09:58[0m] (step=0001221) Train Loss mse: 0.2172, Train Loss ce: 0.3852, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:10:23[0m] (step=0001222) Train Loss mse: 0.1867, Train Loss ce: 0.4286, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:10:56[0m] (step=0001223) Train Loss mse: 0.1893, Train Loss ce: 0.4845, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:11:24[0m] (step=0001224) Train Loss mse: 0.1373, Train Loss ce: 0.3705, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:11:49[0m] (step=0001225) Train Loss mse: 0.1271, Train Loss ce: 0.4603, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:12:09[0m] (step=0001226) Train Loss mse: 0.1345, Train Loss ce: 0.4090, Train Steps/Sec: 0.05, 
[[34m2025-06-23 06:12:41[0m] (step=0001227) Train Loss mse: 0.1992, Train Loss ce: 0.4205, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:13:10[0m] (step=0001228) Train Loss mse: 0.1637, Train Loss ce: 0.4172, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:13:37[0m] (step=0001229) Train Loss mse: 0.1369, Train Loss ce: 0.3853, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:14:04[0m] (step=0001230) Train Loss mse: 0.1647, Train Loss ce: 0.3993, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:14:32[0m] (step=0001231) Train Loss mse: 0.1800, Train Loss ce: 0.4598, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:15:05[0m] (step=0001232) Train Loss mse: 0.2055, Train Loss ce: 0.4249, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:15:38[0m] (step=0001233) Train Loss mse: 0.1777, Train Loss ce: 0.3594, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:16:09[0m] (step=0001234) Train Loss mse: 0.2268, Train Loss ce: 0.3734, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:16:34[0m] (step=0001235) Train Loss mse: 0.1663, Train Loss ce: 0.3752, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:17:01[0m] (step=0001236) Train Loss mse: 0.1917, Train Loss ce: 0.4792, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:17:39[0m] (step=0001237) Train Loss mse: 0.1673, Train Loss ce: 0.4098, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:18:06[0m] (step=0001238) Train Loss mse: 0.1978, Train Loss ce: 0.3496, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:18:35[0m] (step=0001239) Train Loss mse: 0.1373, Train Loss ce: 0.3898, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:19:07[0m] (step=0001240) Train Loss mse: 0.1883, Train Loss ce: 0.4178, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:19:30[0m] (step=0001241) Train Loss mse: 0.1134, Train Loss ce: 0.4647, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:19:57[0m] (step=0001242) Train Loss mse: 0.1990, Train Loss ce: 0.3692, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:20:31[0m] (step=0001243) Train Loss mse: 0.1532, Train Loss ce: 0.3916, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:20:55[0m] (step=0001244) Train Loss mse: 0.1948, Train Loss ce: 0.3747, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:21:20[0m] (step=0001245) Train Loss mse: 0.2257, Train Loss ce: 0.3948, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:21:48[0m] (step=0001246) Train Loss mse: 0.2102, Train Loss ce: 0.4244, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:22:16[0m] (step=0001247) Train Loss mse: 0.1659, Train Loss ce: 0.3548, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:22:45[0m] (step=0001248) Train Loss mse: 0.1444, Train Loss ce: 0.4032, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:23:16[0m] (step=0001249) Train Loss mse: 0.1602, Train Loss ce: 0.3842, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:23:42[0m] (step=0001250) Train Loss mse: 0.1987, Train Loss ce: 0.4138, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:23:43[0m] Saving checkpoint to results/checkpoints/0001250.
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/bagel/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[[34m2025-06-23 06:28:44[0m] Sorted checkpoint directories: ['0001070', '0001100', '0001200', '0001250']
[[34m2025-06-23 06:28:54[0m] Deleted old checkpoint folder: results/checkpoints/0001070
[[34m2025-06-23 06:29:22[0m] (step=0001251) Train Loss mse: 0.1562, Train Loss ce: 0.4266, Train Steps/Sec: 0.00, 
[[34m2025-06-23 06:29:50[0m] (step=0001252) Train Loss mse: 0.1268, Train Loss ce: 0.3668, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:30:14[0m] (step=0001253) Train Loss mse: 0.1093, Train Loss ce: 0.3720, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:30:41[0m] (step=0001254) Train Loss mse: 0.1812, Train Loss ce: 0.3535, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:31:14[0m] (step=0001255) Train Loss mse: 0.2068, Train Loss ce: 0.5574, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:31:45[0m] (step=0001256) Train Loss mse: 0.1504, Train Loss ce: 0.3778, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:32:12[0m] (step=0001257) Train Loss mse: 0.1426, Train Loss ce: 0.3863, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:32:40[0m] (step=0001258) Train Loss mse: 0.1325, Train Loss ce: 0.3955, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:33:10[0m] (step=0001259) Train Loss mse: 0.1733, Train Loss ce: 0.4350, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:33:37[0m] (step=0001260) Train Loss mse: 0.1709, Train Loss ce: 0.4413, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:34:04[0m] (step=0001261) Train Loss mse: 0.1612, Train Loss ce: 0.4037, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:34:30[0m] (step=0001262) Train Loss mse: 0.1171, Train Loss ce: 0.4491, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:34:56[0m] (step=0001263) Train Loss mse: 0.1940, Train Loss ce: 0.4037, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:35:24[0m] (step=0001264) Train Loss mse: 0.1970, Train Loss ce: 0.3339, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:35:51[0m] (step=0001265) Train Loss mse: 0.1724, Train Loss ce: 0.3377, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:36:22[0m] (step=0001266) Train Loss mse: 0.1402, Train Loss ce: 0.3802, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:36:52[0m] (step=0001267) Train Loss mse: 0.1454, Train Loss ce: 0.3821, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:37:23[0m] (step=0001268) Train Loss mse: 0.2071, Train Loss ce: 0.4314, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:38:04[0m] (step=0001269) Train Loss mse: 0.1719, Train Loss ce: 0.3139, Train Steps/Sec: 0.02, 
[[34m2025-06-23 06:38:29[0m] (step=0001270) Train Loss mse: 0.1374, Train Loss ce: 0.3961, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:38:58[0m] (step=0001271) Train Loss mse: 0.2282, Train Loss ce: 0.3808, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:39:33[0m] (step=0001272) Train Loss mse: 0.1781, Train Loss ce: 0.3861, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:40:02[0m] (step=0001273) Train Loss mse: 0.1784, Train Loss ce: 0.4740, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:40:28[0m] (step=0001274) Train Loss mse: 0.1756, Train Loss ce: 0.3718, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:41:05[0m] (step=0001275) Train Loss mse: 0.1895, Train Loss ce: 0.3728, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:41:32[0m] (step=0001276) Train Loss mse: 0.1901, Train Loss ce: 0.4202, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:41:58[0m] (step=0001277) Train Loss mse: 0.1706, Train Loss ce: 0.3098, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:42:23[0m] (step=0001278) Train Loss mse: 0.1233, Train Loss ce: 0.4256, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:42:49[0m] (step=0001279) Train Loss mse: 0.1880, Train Loss ce: 0.3620, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:43:21[0m] (step=0001280) Train Loss mse: 0.1641, Train Loss ce: 0.2848, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:43:50[0m] (step=0001281) Train Loss mse: 0.1732, Train Loss ce: 0.4658, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:44:20[0m] (step=0001282) Train Loss mse: 0.1895, Train Loss ce: 0.3310, Train Steps/Sec: 0.03, 
[[34m2025-06-23 06:44:46[0m] (step=0001283) Train Loss mse: 0.1999, Train Loss ce: 0.4071, Train Steps/Sec: 0.04, 
[[34m2025-06-23 06:45:11[0m] (step=0001284) Train Loss mse: 0.1880, Train Loss ce: 0.4092, Train Steps/Sec: 0.04, 
