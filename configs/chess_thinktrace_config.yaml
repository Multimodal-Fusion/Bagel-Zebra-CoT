# Training configuration for chess ThinkTrace dataset
# Interleaved text and image generation with chain-of-thought reasoning

data:
  grouped_datasets:
    think_trace:
      dataset_names: 
        - chess_thinktrace
      weight: 1.0
      is_mandatory: true
      num_used_data: 
        - 10  # Use all 10 samples from toy dataset
      shuffle_lines: true
      shuffle_seed: 42
      # VAE transform for image generation
      image_transform_args:
        image_stride: 16
        max_image_size: 1024
        min_image_size: 512
      # VIT transform for image understanding
      vit_image_transform_args:
        image_stride: 14
        max_image_size: 980
        min_image_size: 378
        max_pixels: 2007040
  
  # CFG dropout probabilities
  text_cond_dropout_prob: 0.1  # 10% dropout for text CFG
  vae_cond_dropout_prob: 0.1   # 10% dropout for VAE CFG  
  vit_cond_dropout_prob: 0.4   # 40% dropout for VIT CFG
  
  # Image processing parameters
  vae_image_downsample: 16
  max_latent_size: 32
  vit_patch_size: 14
  max_num_patch_per_side: 70
  
  # Packing parameters
  expected_num_tokens: 8192    # Expected tokens per batch
  max_num_tokens: 16384        # Maximum tokens per batch
  max_num_tokens_per_sample: 8192  # Maximum tokens per sample
  prefer_buffer_before: 4096
  max_buffer_size: 10
  interpolate_pos: false
  use_flex: false

training:
  # Model settings
  model_name: "Qwen/Qwen2.5-0.5B-Instruct"
  
  # Training parameters
  batch_size: 1  # Always 1 for packed dataset
  num_workers: 4
  learning_rate: 1e-5
  warmup_steps: 100
  max_steps: 1000
  
  # Loss weights
  ce_loss_weight: 1.0  # Text generation loss
  mse_loss_weight: 1.0  # Image generation loss
  
  # Optimizer
  optimizer: "adamw"
  weight_decay: 0.01
  gradient_clip: 1.0
  
  # Checkpointing
  save_steps: 100
  eval_steps: 50
  logging_steps: 10
  
  # Distributed training
  local_rank: 0
  world_size: 1
  
features:
  # What this configuration enables
  - "Chain-of-thought reasoning with <think> tokens"
  - "Visual reasoning with generated images during thinking"
  - "Interleaved text (CE loss) and image (MSE loss) training"
  - "Triple image representation: VAE loss, VAE conditioning, VIT understanding"
  - "Special token loss for predicting when to generate images"
  - "Problem images as VIT input for understanding"
  - "Reasoning images with both generation loss and understanding"